{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a55f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# import keras.backend as K\n",
    "# from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.regularizers import l1,l2, l1_l2\n",
    "# from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import Layer, Permute, multiply\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Conv1D, Dropout, Dense, BatchNormalization, AveragePooling1D, ReLU, LayerNormalization\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU, Lambda, Reshape, Flatten, SeparableConv1D, Activation, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input, Add, MaxPooling1D, MultiHeadAttention, concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.compat.v1.keras.models import load_model\n",
    "# from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "import sklearn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import class_weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dbsi = pd.read_csv('D:\\Wiltang_2022\\Paper_Data_Codes\\DL_TimeSeries\\Point_Training\\Merge_Training_All_DBSI.csv')\n",
    "data_mndwi = pd.read_csv('D:\\Wiltang_2022\\Paper_Data_Codes\\DL_TimeSeries\\Point_Training\\Merge_Training_All_MNDWI.csv')\n",
    "data_msavi = pd.read_csv('D:\\Wiltang_2022\\Paper_Data_Codes\\DL_TimeSeries\\Point_Training\\Merge_Training_All_MSAVI_rev.csv')\n",
    "# data_ndbi = pd.read_csv('D:\\Wiltang_2022\\Paper_Data_Codes\\DL_TimeSeries\\Point_Training\\Merge_Training_All_NDBI.csv')\n",
    "data_ndvi = pd.read_csv('D:\\Wiltang_2022\\Paper_Data_Codes\\DL_TimeSeries\\Point_Training\\Merge_Training_All_NDVI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57099a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cbi= pd.read_csv('D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Wonosobo_Data/Refined_Extract_Wonosobo/CBI_final_Wonosobo.csv')\n",
    "# data_bsi= pd.read_csv('D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Wonosobo_Data/Refined_Extract_Wonosobo/BSI_final_Wonosobo.csv')\n",
    "# data_evi2= pd.read_csv('D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Wonosobo_Data/Refined_Extract_Wonosobo/EVI2_final_Wonosobo.csv')\n",
    "# data_mndwi= pd.read_csv('D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Wonosobo_Data/Refined_Extract_Wonosobo/MNDWI_final_Wonosobo.csv')\n",
    "# data_msavi= pd.read_csv('D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Wonosobo_Data/Refined_Extract_Wonosobo/MSAVI_final_Wonosobo.csv')\n",
    "# data_nbr= pd.read_csv('D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Wonosobo_Data/Refined_Extract_Wonosobo/NBR_final_Wonosobo.csv')\n",
    "# data_ndvi= pd.read_csv('D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Wonosobo_Data/Refined_Extract_Wonosobo/NDVI_final_Wonosobo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dbsi_ = data_dbsi.values\n",
    "data_mndwi_ = data_mndwi.values\n",
    "data_msavi_ = data_msavi.values\n",
    "# data_ndbi_ = data_ndbi.values\n",
    "data_ndvi_ = data_ndvi.values\n",
    "\n",
    "# dbsi_pd = pd.DataFrame(data_dbsi_)\n",
    "mndwi_pd = pd.DataFrame(data_mndwi_)\n",
    "msavi_pd = pd.DataFrame(data_msavi_)\n",
    "# ndbi_pd = pd.DataFrame(data_ndbi_)\n",
    "ndvi_pd = pd.DataFrame(data_ndvi_)\n",
    "\n",
    "# dbsi_pd  = dbsi_pd.iloc[:,3:42].astype(float)\n",
    "mndwi_pd  = mndwi_pd.iloc[:,3:42].astype(float)\n",
    "msavi_pd = msavi_pd.iloc[:,3:42].astype(float)\n",
    "# ndbi_pd  = ndbi_pd.iloc[:,3:42].astype(float)\n",
    "ndvi_pd  = ndvi_pd.iloc[:,3:42].astype(float)\n",
    "\n",
    "# data_dbsi_int = dbsi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "data_mndwi_int = mndwi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "data_msavi_int = msavi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "# data_ndbi_int = ndbi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "data_ndvi_int = ndvi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70834520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_dbsi_int.shape)\n",
    "print(data_mndwi_int.shape)\n",
    "print(data_msavi_int.shape)\n",
    "# print(data_ndbi_int.shape)\n",
    "print(data_ndvi_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate NaN for each row\n",
    "# data_b2 = data_b2.apply(pd.to_numeric, errors='coerce')\n",
    "# data_b3 = data_b3.apply(pd.to_numeric, errors='coerce')\n",
    "# data_b4 = data_b4.apply(pd.to_numeric, errors='coerce')\n",
    "# data_b5 = data_b5.apply(pd.to_numeric, errors='coerce')\n",
    "# data_b6 = data_b6.apply(pd.to_numeric, errors='coerce')\n",
    "# data_b7 = data_b7.apply(pd.to_numeric, errors='coerce')\n",
    "# data_b8 = data_b8.apply(pd.to_numeric, errors='coerce')\n",
    "# data_b8A = data_b8A.apply(pd.to_numeric, errors='coerce')\n",
    "# data_b11 = data_b11.apply(pd.to_numeric, errors='coerce')\n",
    "# data_b12 = data_b12.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# data_cbi_ = data_cbi.values\n",
    "# data_bsi_ = data_bsi.values\n",
    "# data_evi2_ = data_evi2.values\n",
    "# data_mndwi_ = data_mndwi.values\n",
    "# data_msavi_ = data_msavi.values\n",
    "# data_nbr_ = data_nbr.values\n",
    "# data_ndvi_ = data_ndvi.values\n",
    "\n",
    "# cbi_pd = pd.DataFrame(data_cbi_)\n",
    "# bsi_pd = pd.DataFrame(data_bsi_)\n",
    "# evi2_pd = pd.DataFrame(data_evi2_)\n",
    "# mndwi_pd = pd.DataFrame(data_mndwi_)\n",
    "# msavi_pd = pd.DataFrame(data_msavi_)\n",
    "# nbr_pd = pd.DataFrame(data_nbr_)\n",
    "# ndvi_pd = pd.DataFrame(data_ndvi_)\n",
    "\n",
    "# cbi_pd = cbi_pd.iloc[:,3:42].astype(float)\n",
    "# bsi_pd = bsi_pd.iloc[:,3:42].astype(float)\n",
    "# evi2_pd  = evi2_pd.iloc[:,3:42].astype(float)\n",
    "# mndwi_pd  = mndwi_pd.iloc[:,3:42].astype(float)\n",
    "# msavi_pd = msavi_pd.iloc[:,3:42].astype(float)\n",
    "# nbr_pd  = nbr_pd.iloc[:,3:42].astype(float)\n",
    "# ndvi_pd  = ndvi_pd.iloc[:,3:42].astype(float)\n",
    "\n",
    "# data_b2_int = data_b2.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "# data_b3_int = data_b3.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "# data_b4_int = data_b4.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "# data_b5_int = data_b5.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "# data_b6_int = data_b6.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "# data_b7_int = data_b7.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "# data_b8_int = data_b8.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "# data_b8A_int = data_b8A.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "# data_b11_int = data_b11.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "# data_b12_int = data_b12.interpolate(method='linear', limit_direction='both', axis=1)\n",
    "\n",
    "# data_cbi_int = cbi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "# data_bsi_int = bsi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "# data_evi2_int = evi2_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "# data_mndwi_int = mndwi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "# data_msavi_int = msavi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "# data_nbr_int = nbr_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "# data_ndvi_int = ndvi_pd.interpolate(method='spline', order=3, limit_direction='both', axis=1)\n",
    "\n",
    "# print('Number of nan for B2 after:', data_b2_int.isnull().sum().sum())\n",
    "# print('Number of nan for B3 after:', data_b3_int.isnull().sum().sum())\n",
    "# print('Number of nan for B4 after:', data_b4_int.isnull().sum().sum())\n",
    "# print('Number of nan for B5 after:', data_b5_int.isnull().sum().sum())\n",
    "# print('Number of nan for B6 after:', data_b6_int.isnull().sum().sum())\n",
    "# print('Number of nan for B7 after:', data_b7_int.isnull().sum().sum())\n",
    "# print('Number of nan for B8 after:', data_b8_int.isnull().sum().sum())\n",
    "# print('Number of nan for B8A after:', data_b8A_int.isnull().sum().sum())\n",
    "# print('Number of nan for B11 after:', data_b11_int.isnull().sum().sum())\n",
    "# print('Number of nan for B12 after:', data_b12_int.isnull().sum().sum())\n",
    "\n",
    "# print('Number of nan for CBI after:', np.count_nonzero(np.isnan(data_cbi_int)))\n",
    "# print('Number of nan for BSI after:', np.count_nonzero(np.isnan(data_bsi_int)))\n",
    "# print('Number of nan for EVI2 after:', np.count_nonzero(np.isnan(data_evi2_int)))\n",
    "# print('Number of nan for MNDWI after:', data_mndwi_int.isnull().sum().sum())\n",
    "# print('Number of nan for MSAVI after:', data_msavi_int.isnull().sum().sum())\n",
    "# print('Number of nan for NBR after:', data_nbr_int.isnull().sum().sum())\n",
    "# print('Number of nan for NDVI after:', data_ndvi_int.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b2ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_all = pd.concat([data_dbsi_int, data_mndwi_int, data_msavi_int,\n",
    "#                       data_ndbi_int, data_ndvi_int], axis =0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_all = pd.concat([data_mndwi_int, data_msavi_int,\n",
    "                      data_ndvi_int], axis =0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_all_lc = pd.concat([data_cbi.iloc[:,0:3], data_bsi.iloc[:,0:3], data_evi2.iloc[:,0:3],\n",
    "#                             data_mndwi.iloc[:,0:3], data_msavi.iloc[:,0:3],\n",
    "#                             data_nbr.iloc[:,0:3], data_ndvi.iloc[:,0:3]], axis =0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(comb_all_lc.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_mndwi.iloc[:,0]\n",
    "y = np.asarray(y).astype(np.float32)\n",
    "y_cat = to_categorical(y)\n",
    "y_cat = y_cat[:,1:7] # num_class + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bbde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_cat)\n",
    "print(y_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight = \"balanced\",\n",
    "                                      classes = np.unique(y), y=y)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "# class_weights = dict(zip(np.unique(train_classes), class_weights))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50054c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_b2 = comb_all.iloc[0:1939:,3:31]\n",
    "# comb_b3 = comb_all.iloc[1939:3878:,3:31]\n",
    "# comb_b4 = comb_all.iloc[3878:5817:,3:31]\n",
    "# comb_b5 = comb_all.iloc[5817:7756:,3:31]\n",
    "# comb_b6 = comb_all.iloc[7756:9695:,3:31]\n",
    "# comb_b7 = comb_all.iloc[9695:11634:,3:31]\n",
    "# comb_b8 = comb_all.iloc[11634:13573:,3:31]\n",
    "# comb_b8A = comb_all.iloc[13573:15512:,3:31]\n",
    "# comb_b11 = comb_all.iloc[15512:17451:,3:31]\n",
    "# comb_b12 = comb_all.iloc[17451:19390:,3:31]\n",
    "\n",
    "# comb_dbsi = comb_all.iloc[0:1512:,:]\n",
    "# comb_mndwi = comb_all.iloc[1512:3024:,:]\n",
    "# comb_msavi = comb_all.iloc[3024:4536:,:]\n",
    "# comb_ndbi= comb_all.iloc[4536:6048:,:]\n",
    "# comb_ndvi = comb_all.iloc[6048:7560:,:]\n",
    "\n",
    "comb_mndwi = comb_all.iloc[0:1512:,:]\n",
    "comb_msavi = comb_all.iloc[1512:3024:,:]\n",
    "comb_ndvi = comb_all.iloc[3024:4536:,:]\n",
    "\n",
    "# comb_b2= np.asarray(comb_b2).astype(np.float32)\n",
    "# comb_b3= np.asarray(comb_b3).astype(np.float32)\n",
    "# comb_b4= np.asarray(comb_b4).astype(np.float32)\n",
    "# comb_b5= np.asarray(comb_b5).astype(np.float32)\n",
    "# comb_b6= np.asarray(comb_b6).astype(np.float32)\n",
    "# comb_b7= np.asarray(comb_b7).astype(np.float32)\n",
    "# comb_b8= np.asarray(comb_b8).astype(np.float32)\n",
    "# comb_b8A= np.asarray(comb_b8A).astype(np.float32)\n",
    "# comb_b11= np.asarray(comb_b11).astype(np.float32)\n",
    "# comb_b12= np.asarray(comb_b12).astype(np.float32)\n",
    "\n",
    "# comb_dbsi= np.asarray(comb_dbsi).astype(np.float64)\n",
    "comb_mndwi= np.asarray(comb_mndwi).astype(np.float64)\n",
    "comb_msavi= np.asarray(comb_msavi).astype(np.float64)\n",
    "# comb_ndbi= np.asarray(comb_ndbi).astype(np.float64)\n",
    "comb_ndvi= np.asarray(comb_ndvi).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d418916",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comb_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec61a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_stack = np.stack((comb_dbsi, comb_mndwi, comb_msavi,\n",
    "#                        comb_ndbi, comb_ndvi), axis=1)\n",
    "\n",
    "comb_stack = np.stack((comb_mndwi, comb_msavi,\n",
    "                       comb_ndvi), axis=1)\n",
    "\n",
    "# comb_stack = np.stack((comb_cbi, comb_bsi, comb_evi2), axis=1)\n",
    "# print(comb_stack.shape)\n",
    "\n",
    "comb_stack_reshape = comb_stack.reshape(comb_stack.shape[0], comb_stack.shape[2], comb_stack.shape[1])\n",
    "print(comb_stack_reshape.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(comb_stack_reshape , y_cat, test_size=0.30, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ab41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(comb_cbi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b594aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack_ver = np.hstack((vert_cbi, vert_bsi, vert_evi2,\n",
    "#                        vert_mndwi, vert_msavi, vert_nbr, \n",
    "#                        vert_ndvi))\n",
    "# print(stack_ver.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns \n",
    "\n",
    "# x = ['CBI','BSI','EVI2', 'MNDWI', 'MSAVI', 'NBR', 'NDVI']\n",
    "# # enumerate label\n",
    "# for i in enumerate(x):\n",
    "#     print(i)\n",
    "\n",
    "# # plot correlation matrix\n",
    "# rf_pd = pd.DataFrame(data=stack_ver, columns=list(x))\n",
    "# matrix = np.triu(rf_pd.corr())\n",
    "# plt.figure(figsize=(12,8))\n",
    "# sns.heatmap(rf_pd.corr(), annot=True, mask=matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa061434",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "print(num_classes)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d552ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'swish': swish})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bda406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReshapeLayer(x):\n",
    "    shape = x.shape\n",
    "    # 1 possibility: H,W*channel\n",
    "    reshape = Reshape((shape[1],shape[2]))(x)\n",
    "    # 2 possibility: W,H*channel\n",
    "    # transpose = Permute((2,1,3))(x)\n",
    "    # reshape = Reshape((shape[1],shape[2]*shape[3]))(transpose)\n",
    "    return reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs, output_shape):\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(output_shape, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af8334",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#2nd best\n",
    "\n",
    "# def make_model(input_shape):\n",
    "#     input_layer = Input(input_shape)\n",
    "    \n",
    "# #     x = LayerNormalization()(input_layer)\n",
    "# #     x = MultiHeadAttention(key_dim=128, num_heads=2, dropout=0)(x, x)\n",
    "# #     x = Dropout(0.5)(x)\n",
    "    \n",
    "#     np.random.seed(16)\n",
    "#     tf.random.set_seed(16)\n",
    "    \n",
    "#     init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=0)\n",
    "    \n",
    "#     x0 = Conv1D(64, 1, strides=1, activation=\"swish\", padding = 'same', dilation_rate=1, kernel_initializer=init)(input_layer)\n",
    "#     x0 = Conv1D(64, 1, strides=1, activation=\"swish\", padding = 'same', dilation_rate=1, kernel_initializer=init)(x0)\n",
    "#     x0 = LayerNormalization()(x0)\n",
    "#     x0 = Dropout(0.5)(x0)\n",
    "    \n",
    "#     x01 = Conv1D(64, 1, activation=\"swish\", padding = 'same', dilation_rate=1, kernel_initializer=init)(x0)\n",
    "#     x01 = Conv1D(64, 1, activation=\"swish\", padding = 'same', dilation_rate=1, kernel_initializer=init)(x01)\n",
    "#     x01 = LayerNormalization()(x01)\n",
    "#     x01 = Dropout(0.5)(x01)\n",
    "    \n",
    "#     x02 = Conv1D(64, 1, activation=\"swish\", padding = 'same', dilation_rate=1, kernel_initializer=init)(x01)\n",
    "#     x02 = Conv1D(64, 1, activation=\"swish\", padding = 'same', dilation_rate=1, kernel_initializer=init)(x02)\n",
    "#     x02 = LayerNormalization()(x02)\n",
    "#     x02 = Dropout(0.5)(x02)\n",
    "    \n",
    "#     add_ = tf.keras.layers.add([x0, x02])\n",
    "    \n",
    "#     xfin = Lambda(ReshapeLayer)(add_)\n",
    "#     xfin = Bidirectional(GRU(64, activation='tanh', return_sequences=False))(xfin)\n",
    "# #     xfin = Dense(128, activation=\"swish\")(xfin)\n",
    "# #     xfin = Dense(64, activation=\"swish\")(xfin)\n",
    "#     xfin = LayerNormalization()(xfin)\n",
    "#     xfin = Dropout(0.75)(xfin)\n",
    "\n",
    "#     output_layer = Dense(num_classes, activation=\"softmax\")(xfin)\n",
    "\n",
    "#     return Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# model = make_model(input_shape= input_shape)\n",
    "# model.summary()\n",
    "# model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#                     optimizer=optimizers.Adam(learning_rate=0.001), \n",
    "#                     metrics=['accuracy'])\n",
    "\n",
    "# checkpoint = ModelCheckpoint(filepath='D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Saved_Model/LC_DAS_Citarum_exp03_stable.h5',\n",
    "#                               monitor='val_accuracy',save_best_only=True,verbose=1)\n",
    "# early_stop = EarlyStopping(monitor='val_accuracy',patience=100,restore_best_weights=True,mode='max')\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,patience=30, min_lr=0.00001)\n",
    "\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size =20,\n",
    "#                     epochs=500, callbacks = [checkpoint, early_stop, reduce_lr], \n",
    "#                     class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb6043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Best set up\n",
    "\n",
    "def make_model(input_shape):\n",
    "    input_layer = Input(input_shape)\n",
    "    \n",
    "#     x = LayerNormalization()(input_layer)\n",
    "#     x = MultiHeadAttention(key_dim=64, num_heads=2, dropout=0.5)(x, x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "    \n",
    "#     np.random.seed(16)\n",
    "#     tf.random.set_seed(16)\n",
    "    \n",
    "#     init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=0)\n",
    "        \n",
    "    x0a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(input_layer)\n",
    "    x0a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x0a)\n",
    "    \n",
    "    x01a = Conv1D(16, 3, activation=\"swish\", padding = 'same')(x0a)\n",
    "    x01a = Conv1D(16, 3, activation=\"swish\", padding = 'same')(x01a)\n",
    "    \n",
    "    x02 = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x01a)\n",
    "    x02 = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x02)\n",
    "    \n",
    "    add_1 = tf.keras.layers.add([x0a, x02])\n",
    "    add_1 = LayerNormalization()(add_1)\n",
    "    \n",
    "    x03 = Conv1D(16, 3, activation=\"swish\", padding = 'same')(add_1)\n",
    "    \n",
    "    x04 = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x03)\n",
    "    x04 = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x04)\n",
    "    x04 = LayerNormalization()(x04)\n",
    "    \n",
    "    xfin = Lambda(ReshapeLayer)(x04)\n",
    "    xfin = Bidirectional(GRU(64, activation='tanh', return_sequences=False))(xfin)\n",
    "    xfin = Dropout(0.5)(xfin)\n",
    "\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(xfin)\n",
    "\n",
    "    return Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model = make_model(input_shape= input_shape)\n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=optimizers.AdamW(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Saved_Model/LC_DAS_Citarum_exp03_stable_WonosoboROI.h5',\n",
    "                              monitor='val_accuracy',save_best_only=True,verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy',patience=100,restore_best_weights=True,mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,patience=30, min_lr=0.00001)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size =20,\n",
    "                    epochs=500, callbacks = [checkpoint, early_stop, reduce_lr], \n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = len(history.history['loss'])\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(n_epochs) #change it based on epoch needed to finish building the model\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c490ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "labels, counts = np.unique(y, return_counts=True)\n",
    "labels = labels -1\n",
    "classes = ['Urban','Dense Vegetation','Agriculture', 'Plantation', 'Waterbody', 'Bareland']\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "# plot the confusion matrix of LGB\n",
    "dl_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "im = ax.imshow(dl_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "\n",
    "# Show all labels\n",
    "ax.set(xticks=np.arange(dl_cm.shape[1]),\n",
    "        yticks=np.arange(dl_cm.shape[0]),\n",
    "        # ... and label them with the respective list entries\n",
    "        xticklabels=classes, yticklabels=classes,\n",
    "        title='Normalized Confusion Matrix',\n",
    "        ylabel='True label',\n",
    "        xlabel='Predicted label')\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "          rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "fmt = '.2f'\n",
    "thresh = dl_cm.max() / 2.\n",
    "for i in range(dl_cm.shape[0]):\n",
    "    for j in range(dl_cm.shape[1]):\n",
    "        ax.text(j, i, format(dl_cm[i, j], fmt),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if dl_cm[i, j] > thresh else \"black\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric Precision, Recall, F-score support\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "macro_01 = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "weighted_01 = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(macro_01)\n",
    "print(weighted_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a28b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric F1-score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "macro_02 = f1_score(y_test, y_pred, average='macro')\n",
    "weighted_02 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(macro_02)\n",
    "print(weighted_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769bcca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the precision and recall, among other metrics\n",
    "from sklearn import metrics\n",
    "from tensorflow.compat.v1.keras.models import load_model\n",
    "\n",
    "# model = load_model('D:/Wiltang_2022/Paper_Data_Codes/Saved_Model/LC_Multivariate_Wonosobo_BiGRU_exp_ver01_metricmeasurement.h5')\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "# y_test = y_test.argmax(axis=1)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train = y_pred_train.argmax(axis=1)\n",
    "y_train = y_train.argmax(axis=1)\n",
    "\n",
    "JS_test = jaccard_score(y_test, y_pred, average='macro')\n",
    "JS_train = jaccard_score(y_train, y_pred_train, average='macro')\n",
    "print(JS_test)\n",
    "print(JS_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6200bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train = y_pred_train.argmax(axis=1)\n",
    "# y_train = y_train.argmax(axis=1)\n",
    "\n",
    "JS_test = jaccard_score(y_test, y_pred, average=None)\n",
    "JS_train = jaccard_score(y_train, y_pred_train, average=None)\n",
    "print(JS_test)\n",
    "print(JS_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23744261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "model_kappa = cohen_kappa_score(y_pred, y_test)\n",
    "print(model_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test = {'ytest': y_test,\n",
    "#         'ytest_pred': y_pred}\n",
    "\n",
    "# data_train = {'ytest': y_train,\n",
    "#         'ytest_pred': y_pred_train}\n",
    "\n",
    "# df_test = pd.DataFrame(data_test)\n",
    "# df_train = pd.DataFrame(data_train)\n",
    "\n",
    "# df_test.to_csv('D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/df_test.csv')\n",
    "# df_train.to_csv('D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/df_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65ab91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# import pydot\n",
    "# import pydotplus\n",
    "# from pydotplus import graphviz\n",
    "\n",
    "# plot_model(model, to_file='D:/DL_DASCitarum/Spasial_Vector/DAS_Citarum_Extract_Landsat/Saved_Model/model3_plot.png', \n",
    "#            show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64f4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
